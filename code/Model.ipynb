{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import gc\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import choice, seed, shuffle, random, sample\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, CuDNNGRU as GRU, CuDNNLSTM as LSTM, Dropout, BatchNormalization\n",
    "# from keras.layers import Input, GRU, LSTM, Dropout, BatchNormalization\n",
    "from keras.layers import Dense, Concatenate, Activation, Embedding, SpatialDropout1D, Bidirectional, Lambda, Conv1D\n",
    "from keras.layers import Add, Average, TimeDistributed, GlobalMaxPooling1D\n",
    "from keras.optimizers import Nadam, Adam, Adamax\n",
    "from keras.activations import absolute_import\n",
    "from keras.legacy import interfaces\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import Callback\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.initializers import he_normal\n",
    "# from keras_bert.bert import get_model\n",
    "from keras_bert.loader import load_trained_model_from_checkpoint\n",
    "from keras_bert import AdamWarmup, calc_train_steps\n",
    "from keras.engine import Layer\n",
    "from keras.engine import InputSpec\n",
    "from keras.objectives import categorical_crossentropy\n",
    "from keras.objectives import sparse_categorical_crossentropy\n",
    "from keras import activations, initializers, regularizers, constraints\n",
    "from keras.models import Model\n",
    "from tqdm import tqdm\n",
    "from model_utils import seq_gather, seq_and_vec, seq_maxpool\n",
    "from keras.models import load_model\n",
    "from keras_bert import get_custom_objects\n",
    "from keras_bert import Tokenizer\n",
    "from collections import defaultdict\n",
    "from eval import read_submission, get_ndcg\n",
    "from tqdm import tqdm, trange\n",
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True \n",
    "sess = tf.Session(config=config)\n",
    "KTF.set_session(sess)\n",
    "\n",
    "BERT_PRETRAINED_DIR = \"../data/\"\n",
    "VAL_ANS_PATH = '../data/valid_answer.json'\n",
    "LABEL_PATH = '../data/multimodal_labels.txt'\n",
    "\n",
    "MAX_EPOCH = 6\n",
    "MAX_LEN = 20\n",
    "B_SIZE = 128  #128\n",
    "FOLD_IDS = [-1]\n",
    "FOLD_NUM = 20\n",
    "THRE = 0.5\n",
    "SHUFFLE = True\n",
    "MAX_BOX = 10\n",
    "MAX_CHAR = 8\n",
    "PREFIX = \"model\"\n",
    "SEED = 2020\n",
    "ACCUM_STEP = int(128 // B_SIZE)\n",
    "SAVE_EPOCHS=[10, 20, 35, 50, 80, 100]\n",
    "IMAGE_LABEM_CONCAT_TOKEN = \"###\"\n",
    "CONCAT_TOKE = \"[unused0]\"\n",
    "\n",
    "cfg = {}\n",
    "cfg[\"base_dir\"] = BERT_PRETRAINED_DIR\n",
    "cfg['maxlen'] = MAX_LEN\n",
    "cfg[\"max_box\"] = MAX_BOX\n",
    "cfg[\"max_char\"] = MAX_CHAR\n",
    "cfg[\"lr\"] = 1e-4  ### 10 times\n",
    "cfg['min_lr'] = 6e-8\n",
    "cfg[\"opt\"] = \"nadam\"\n",
    "cfg[\"loss_w\"] =  20.\n",
    "cfg[\"trainable\"] = True\n",
    "cfg[\"bert_trainable\"] = True\n",
    "cfg[\"mix_mode\"] = \"\"   # add concat average\n",
    "cfg[\"unit1_1\"] = 128\n",
    "cfg[\"accum_step\"] = ACCUM_STEP\n",
    "cfg[\"cls_num\"] = 4\n",
    "cfg[\"filename\"] = PREFIX\n",
    "\n",
    "num_parts = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5303510655162752383\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 20299774034\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 2025302450037300215\n",
      "physical_device_desc: \"device: 0, name: Quadro RTX 6000, pci bus id: 0000:17:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab():\n",
    "    \n",
    "    dict_path = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\n",
    "    with open(dict_path, mode=\"r\", encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [l.strip() for l in lines]\n",
    "\n",
    "    word_index = {v: k  for k, v in enumerate(lines)}\n",
    "    return word_index\n",
    "\n",
    "\n",
    "word_index = get_vocab()\n",
    "cfg[\"x_pad\"] = word_index[\"[PAD]\"]\n",
    "tokenizer = Tokenizer(word_index)\n",
    "\n",
    "\n",
    "def get_label(path):\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        label2id = {l.split('\\n')[0].split('\\t')[1]:int(l.split('\\n')[0].split('\\t')[0]) for l in lines[1:]}\n",
    "        id2label = {int(l.split('\\n')[0].split('\\t')[0]):l.split('\\n')[0].split('\\t')[1] for l in lines[1:]}\n",
    "    return label2id, id2label\n",
    "\n",
    "\n",
    "label2id, id2label = get_label(LABEL_PATH)\n",
    "label_set = set(label2id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_list = list()\n",
    "\n",
    "for i in range(1, num_parts+1):\n",
    "    print(f\"reading {i}...\")\n",
    "    with open(f'../data/temp_data_{i}.pkl', 'rb') as outp:\n",
    "        train_list.append(joblib.load(outp))\n",
    "\n",
    "print(\"concating...\")\n",
    "train_data = pd.concat(train_list)\n",
    "\n",
    "print(\"reset index...\")\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "train_data.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "print(\"Done reading files at\", datetime.datetime.now()) \n",
    "\n",
    "## start at 10h23 PM Jun 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/val_data.pkl', 'rb') as outp:\n",
    "    val_data = pickle.load(outp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_data_ = len(train_data)\n",
    "train_data.shape, val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype=np.float16)\n",
    "\n",
    "\n",
    "def load_embed(path, dim=300, word_index=None):\n",
    "    embedding_index = {}\n",
    "    with open(path, mode=\"r\", encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for l in lines:\n",
    "            l = l.strip().split()\n",
    "            word, arr = l[0], l[1:]\n",
    "            if len(arr) != dim:\n",
    "                print(\"[!] l = {}\".format(l))\n",
    "                continue\n",
    "            if word_index and word not in word_index:\n",
    "                continue\n",
    "            word, arr = get_coefs(word, arr)\n",
    "            embedding_index[word] = arr\n",
    "    return embedding_index\n",
    "\n",
    "\n",
    "def build_matrix(path, word_index=None, max_features=None, dim=300):\n",
    "    embedding_index = load_embed(path, dim=dim, word_index=word_index)\n",
    "    max_features = len(word_index) + 1 if max_features is None else max_features \n",
    "    embedding_matrix = np.zeros((max_features + 1, dim))\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i <= max_features:\n",
    "            try:\n",
    "                embedding_matrix[i] = embedding_index[word]\n",
    "            except KeyError:\n",
    "#                 print(word)\n",
    "                unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words\n",
    "\n",
    "\n",
    "def load_word_embed(word_embed_glove=\"../data/glove.840B.300d.txt\", \n",
    "                    word_embed_crawl=\"../data/crawl-300d-2M.vec\",\n",
    "               save_filename=\"./word_embedding_matrix\",\n",
    "               word_index=None):\n",
    "    \"\"\"\n",
    "    (30524, 300) 7590\n",
    "    (30524, 300) 7218\n",
    "    \"\"\"    \n",
    "    if os.path.exists(save_filename + \".npy\"):\n",
    "        word_embedding_matrix = np.load(save_filename + \".npy\").astype(\"float32\")\n",
    "    else:\n",
    "        word_embedding_matrix, tx_unk = build_matrix(word_embed_glove, word_index=word_index, dim=300)\n",
    "\n",
    "        print(word_embedding_matrix.shape, len(tx_unk))\n",
    "        \n",
    "        word_embedding_matrix_v2, tx_unk = build_matrix(word_embed_crawl, word_index=word_index, dim=300)\n",
    "\n",
    "        print(word_embedding_matrix_v2.shape, len(tx_unk))\n",
    "        \n",
    "        word_embedding_matrix = np.concatenate([word_embedding_matrix, word_embedding_matrix_v2], axis=1)\n",
    "        \n",
    "        gc.collect()\n",
    "        np.save(save_filename, word_embedding_matrix)\n",
    "    return word_embedding_matrix\n",
    "\n",
    "\n",
    "word_embedding_matrix = load_word_embed(word_index=word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(cfg, summary=False, word_embedding_matrix=None):\n",
    "    \n",
    "    def _get_model(base_dir, cfg_=None):\n",
    "        config_file = os.path.join(base_dir, 'bert_config.json')\n",
    "        checkpoint_file = os.path.join(base_dir, 'bert_model.ckpt')\n",
    "        if not os.path.exists(config_file):\n",
    "            config_file = os.path.join(base_dir, 'bert_config_large.json')\n",
    "            checkpoint_file = os.path.join(base_dir, 'roberta_l24_large_model')\n",
    "        print(config_file, checkpoint_file)\n",
    "#         model = load_trained_model_from_checkpoint(config_file, checkpoint_file, training=True, seq_len=cfg_['maxlen'])\n",
    "        model = load_trained_model_from_checkpoint(config_file, \n",
    "                                           checkpoint_file, \n",
    "                                           training=False, \n",
    "                                           trainable=cfg_[\"bert_trainable\"], \n",
    "                                           output_layer_num=cfg[\"cls_num\"],\n",
    "                                           seq_len=cfg_['maxlen'])\n",
    "        return model\n",
    "    \n",
    "    def get_opt(num_example, warmup_proportion=0.1, lr=2e-5, min_lr=None):\n",
    "        if cfg[\"opt\"].lower() == \"nadam\":\n",
    "            opt = Nadam(lr=lr)\n",
    "            print(\"------\\n Using nadam\")\n",
    "        else:\n",
    "            total_steps, warmup_steps = calc_train_steps(\n",
    "                num_example=num_example,\n",
    "                batch_size=B_SIZE,\n",
    "                epochs=MAX_EPOCH,\n",
    "                warmup_proportion=warmup_proportion,\n",
    "            )\n",
    "\n",
    "            opt = AdamWarmup(total_steps, warmup_steps, lr=lr, min_lr=min_lr)\n",
    "            print(\"------\\n Using AdamWarmup\")\n",
    "\n",
    "            \n",
    "\n",
    "        return opt\n",
    "\n",
    "    model1 = _get_model(cfg[\"base_dir\"], cfg)\n",
    "    model1 = Model(inputs=model1.inputs[: 2], outputs=model1.layers[-7].output)\n",
    "\n",
    "    if word_embedding_matrix is not None:\n",
    "        embed_layer = Embedding(input_dim=word_embedding_matrix.shape[0], \n",
    "                                output_dim=word_embedding_matrix.shape[1],\n",
    "                                weights=[word_embedding_matrix],\n",
    "                                trainable=cfg[\"trainable\"],\n",
    "                                name=\"embed_layer\"\n",
    "                         )\n",
    "        \n",
    "    inp_token1 = Input(shape=(None, ), dtype=np.int32, name=\"query_token_input\")\n",
    "    inp_segm1 = Input(shape=(None, ), dtype=np.float32, name=\"query_segm_input\")\n",
    "    \n",
    "#     inp_token2 = Input(shape=(None, ), dtype=np.int32)\n",
    "#     inp_segm2 = Input(shape=(None, ), dtype=np.float32)    \n",
    "    \n",
    "    inp_image = Input(shape=(None, 2048), dtype=np.float32, name=\"image_input\")\n",
    "    inp_image_mask = Input(shape=(None, ), dtype=np.float32, name=\"image_mask_input\")\n",
    "    inp_pos = Input(shape=(None, 5), dtype=np.float32, name=\"image_pos_input\")        \n",
    "    inp_image_char = Input(shape=(None, cfg[\"max_char\"]), dtype=np.int32, name='image_char_input')\n",
    "    \n",
    "    \n",
    "    mask = Lambda(lambda x: K.cast(K.not_equal(x, cfg[\"x_pad\"]), 'float32'), name=\"token_mask\")(inp_token1)\n",
    "    word_embed = embed_layer(inp_token1)\n",
    "    word_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([word_embed, mask])\n",
    "    word_embed = Bidirectional(LSTM(cfg[\"unit1_1\"], return_sequences=True), merge_mode=\"sum\")(word_embed)\n",
    "    word_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([word_embed, mask])\n",
    "\n",
    "    sequence_output = model1([inp_token1, inp_segm1])\n",
    "    sequence_output = Concatenate(axis=-1)([sequence_output, word_embed])\n",
    "    text_pool = Lambda(lambda x: x[:, 0, :])(sequence_output)\n",
    "\n",
    "    # Share weights of character-level embedding for premise and hypothesis\n",
    "    character_embedding_layer = TimeDistributed(Sequential([\n",
    "        embed_layer,\n",
    "        # Embedding(input_dim=100, output_dim=char_embedding_size, input_length=chars_per_word),\n",
    "        Conv1D(filters=128, kernel_size=3, name=\"char_embed_conv1d\"),\n",
    "        GlobalMaxPooling1D()\n",
    "    ]), name='CharEmbedding')\n",
    "    character_embedding_layer.build(input_shape=(None, None, cfg[\"max_char\"]))\n",
    "    image_char_embed  = character_embedding_layer(inp_image_char)    \n",
    "    image_embed = Concatenate(axis=-1)([image_char_embed, inp_image])    \n",
    "    image_embed = Dense(512, activation='relu', name='image_embed')(image_embed)\n",
    "    image_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([image_embed, inp_image_mask])\n",
    "    pos_embed = Dense(512, activation='relu', name='pos_embed')(inp_pos)\n",
    "    pos_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([pos_embed, inp_image_mask])\n",
    "    embed = Add()([image_embed , pos_embed]) # batch, maxlen(10), 1024+128\n",
    "    \n",
    "    image_embed = Bidirectional(LSTM(1152, return_sequences=True), merge_mode=\"sum\")(embed)\n",
    "    image_embed = Lambda(lambda x: x[0] * K.expand_dims(x[1], axis=-1))([image_embed, inp_image_mask])\n",
    "    \n",
    "    image_pool = Lambda(lambda x: x[:, 0, :])(image_embed)\n",
    "    \n",
    "    pool = Concatenate(axis=-1)([image_pool, text_pool])\n",
    "    pool = Dense(2048, activation=\"relu\")(pool)\n",
    "    pool = Dense(512, activation=\"relu\")(pool)\n",
    "    pool = Dense(128, activation=\"relu\")(pool)\n",
    "    \n",
    "    output = Dense(2, activation='softmax', name='output')(pool)\n",
    "\n",
    "    opt = get_opt(num_example=cfg[\"num_example\"], lr=cfg[\"lr\"], min_lr=cfg['min_lr'])\n",
    "    model = Model(inputs=[inp_token1, inp_segm1, \n",
    "                          inp_image, inp_image_mask,\n",
    "                          inp_pos, inp_image_char], outputs=[output])#\n",
    "    \n",
    "    model.compile(optimizer=opt, loss={\n",
    "                'output': 'sparse_categorical_crossentropy'\n",
    "            }, metrics=['accuracy'])\n",
    "    if summary:\n",
    "        model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# cfg[\"num_example\"] = 32\n",
    "# model = build_model(cfg, summary=True, word_embedding_matrix=word_embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            print(len(tokens_a))\n",
    "            tokens_b.pop()\n",
    "\n",
    "\n",
    "def token2id_X(X, x_dict, maxlen=None):\n",
    "    x = tokenizer.tokenize(X)\n",
    "    if maxlen:\n",
    "        x = x[: 1] + list(x)[1: maxlen - 1] + x[-1: ]     \n",
    "    seg = [0 for _ in x]\n",
    "    token = list(x)\n",
    "    x = [x_dict[e] if e in x_dict else x_dict[\"[UNK]\"] for e in token]\n",
    "    assert len(x) == len(seg)\n",
    "    return x, seg\n",
    "\n",
    "\n",
    "def seq_padding(X, maxlen=None, padding_value=None, debug=False):\n",
    "    L = [len(x) for x in X]\n",
    "    if maxlen is None:\n",
    "        maxlen = max(L)\n",
    "\n",
    "    pad_X = np.array([\n",
    "        np.concatenate([x, [padding_value] * (maxlen - len(x))]) if len(x) < maxlen else x[: maxlen] for x in X\n",
    "    ])\n",
    "    if debug:\n",
    "        print(\"[!] before pading {}\\n\".format(X))\n",
    "        print(\"[!] after pading {}\\n\".format(pad_X))\n",
    "    return pad_X\n",
    "    \n",
    "\n",
    "def MyChoice(Myset):\n",
    "    result = []\n",
    "    for i in Myset:\n",
    "        temp_set = set()\n",
    "        temp_set.add(i)\n",
    "        cho = choice(list(Myset - temp_set))\n",
    "        result.append(cho)\n",
    "    return result\n",
    "\n",
    "\n",
    "class data_generator:\n",
    "    \n",
    "    def __init__(self, data, batch_size=B_SIZE, shuffle=SHUFFLE):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = len(self.data) // self.batch_size\n",
    "        self.shuffle = shuffle\n",
    "        if len(self.data) % self.batch_size != 0:\n",
    "            self.steps += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        inp_token1,\n",
    "        inp_segm1,\n",
    "        inp_image,\n",
    "        inp_image_mask,\n",
    "        inp_pos, \n",
    "        inp_image_char\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            idxs = list(range(len(self.data)))\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(idxs)\n",
    "            T1, T2, Image1, Pos1, label_word_list, image1_mask, image1_char = [], [], [], [], [], [], []\n",
    "            S1, S2, Image2, Pos2, image2_mask, image2_char = [], [], [], [], [], [] \n",
    "            Id_set = set()\n",
    "\n",
    "            for i in idxs:\n",
    "                d = self.data.iloc[i]\n",
    "                text = d['words']\n",
    "                label_words = d['label_words']\n",
    "                \n",
    "                t1, t2 = token2id_X(text, x_dict=word_index, maxlen=cfg[\"maxlen\"])\n",
    "                image = np.array(d['features'], dtype=\"float32\")\n",
    "                image = image[: cfg[\"max_box\"]]\n",
    "                img_mask = [1 for _ in image[: cfg[\"max_box\"]]]\n",
    "                \n",
    "                pos = np.array(d['pos'], dtype=\"float32\")\n",
    "                pos = pos[: cfg[\"max_box\"]]\n",
    "                \n",
    "                image_char = [token2id_X(ent, x_dict=word_index)[0] for ent in label_words.split(IMAGE_LABEM_CONCAT_TOKEN)]\n",
    "                image_char = image_char[: cfg[\"max_box\"]]\n",
    "                # print(\"image_char\", len(image_char))\n",
    "                image_char = pad_sequences(image_char, \n",
    "                                           maxlen=cfg[\"max_char\"], \n",
    "                                           dtype='int32',\n",
    "                                           padding='post',\n",
    "                                           truncating='post',\n",
    "                                           value=cfg[\"x_pad\"])\n",
    "                \n",
    "                assert image.shape[0] == pos.shape[0]\n",
    "                assert image.shape[0] == cfg[\"max_box\"] or image.shape[0] == len(label_words.split(IMAGE_LABEM_CONCAT_TOKEN))\n",
    "                assert image_char.shape == (image.shape[0], cfg[\"max_char\"])\n",
    "\n",
    "                T1.append(t1)\n",
    "                T2.append(t2)\n",
    "                Image1.append(image)\n",
    "                image1_mask.append(img_mask)  \n",
    "                Pos1.append(pos)\n",
    "                image1_char.append(image_char)\n",
    "                Id_set.add(i)\n",
    "\n",
    "                if len(T1) == self.batch_size//2 or i == idxs[-1]:\n",
    "                \n",
    "                    Id_new = MyChoice(Id_set)\n",
    "#                     print(Id_set, Id_new)\n",
    "                    for i, id_ in enumerate(Id_new):\n",
    "                        d_new = self.data.iloc[id_]\n",
    "                        text = d_new['words']\n",
    "                        t1, t2 = token2id_X(text, x_dict=word_index, maxlen=cfg[\"maxlen\"])\n",
    "                        S1.append(t1)\n",
    "                        S2.append(t2)\n",
    "                        \n",
    "                        image = Image1[i]\n",
    "                        img_mask = image1_mask[i]\n",
    "                        pos = Pos1[i]\n",
    "                        image_char = image1_char[i]\n",
    "                        \n",
    "                        Image2.append(image)\n",
    "                        Pos2.append(pos)\n",
    "                        image2_mask.append(img_mask)\n",
    "                        image2_char.append(image_char)\n",
    "                    \n",
    "                    Y = [1] * len(T1) + [0] * len(S1)\n",
    "                   \n",
    "                    T1 = seq_padding(T1 + S1, padding_value=cfg[\"x_pad\"]) \n",
    "                    T2 = seq_padding(T2 + S2, padding_value=cfg[\"x_pad\"])\n",
    "                    \n",
    "                    Image1 = seq_padding(Image1 + Image2, \n",
    "                                         padding_value=np.zeros(shape=(2048, ))\n",
    "                                        )\n",
    "                                                         \n",
    "                    Pos1 = seq_padding(Pos1 + Pos2,\n",
    "                                       padding_value=np.zeros(shape=(5, ))\n",
    "                                      )\n",
    "                    image1_mask = seq_padding(image1_mask + image2_mask,\n",
    "                                             padding_value=0)\n",
    "                    \n",
    "                    image1_char = seq_padding(image1_char + image2_char,\n",
    "                                             padding_value=np.zeros(shape=(cfg[\"max_char\"])), debug=False)\n",
    "                    \n",
    "                    Y = np.array(Y).reshape((len(T1), -1))\n",
    "                    \n",
    "                    idx = np.arange(len(T1))\n",
    "                    np.random.shuffle(idx)\n",
    "        \n",
    "                    T1 = T1[idx]\n",
    "                    T2 = T2[idx]\n",
    "                    Image1 = Image1[idx]\n",
    "                    image1_mask = image1_mask[idx]\n",
    "                    Pos1 = Pos1[idx]\n",
    "                    image1_char = image1_char[idx]\n",
    "                    Y = Y[idx]\n",
    "                    \n",
    "                    yield [T1, T2, Image1, image1_mask, Pos1, image1_char], Y\n",
    "                    T1, T2, Image1, Pos1, label_word_list, image1_mask, image1_char = [], [], [], [], [], [], []\n",
    "                    S1, S2, Image2, Pos2, image2_mask, image2_char = [], [], [], [], [], [] \n",
    "                    Id_set = set()\n",
    "\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _i  = 0\n",
    "# for d in train_D:\n",
    "#     _i += 1\n",
    "#     if  _i > 10:\n",
    "#         print(\"------\")\n",
    "#         print(\"d ===\", d)\n",
    "#         break\n",
    "#     print('x',d[0][0].shape, d[0][1].shape,d[0][2].shape, d[0][3].shape, d[0][4].shape, d[0][5].shape, d[1].shape)\n",
    "#     print(d[0][5].sum(axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluate(Callback):\n",
    "    def __init__(self, filename=None):\n",
    "        self.score = []\n",
    "        self.best = 0.\n",
    "        self.filename = filename\n",
    "       \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch ==  0:\n",
    "            print(\"[!] test load&save model\")\n",
    "            f = self.filename + \".h5\"\n",
    "            custom_objects = get_custom_objects()\n",
    "            self.model.save(f, include_optimizer=False, overwrite=True)\n",
    "            try:\n",
    "                model_ = load_model(f, custom_objects=custom_objects)  \n",
    "            except:\n",
    "                model_ = load_model(f) \n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "#         if epoch + 1 < 5:\n",
    "#             return\n",
    "        score = self.evaluate(self.model)\n",
    "        self.score.append((epoch, score))\n",
    "        \n",
    "        \n",
    "        self.model.save(self.filename + \"_{}.h5\".format(epoch + 1 + 2), include_optimizer=False)             \n",
    "        if score > self.best:\n",
    "            self.model.save(self.filename + \"best_model.h5\", include_optimizer=False)\n",
    "            \n",
    "        if score > self.best:\n",
    "            self.best = score\n",
    "            print(\"[!] epoch = {}, new best score = {}\".format(epoch + 1,  score))\n",
    "        print('[!] epoch = {}, score = {}, best score: {}\\n'.format(epoch + 1, score, self.best))\n",
    "\n",
    "    def evaluate(self, model):\n",
    "        result = defaultdict(list)\n",
    "        for i in trange(len(val_data)):\n",
    "            d = val_data.iloc[i]\n",
    "            qid = d['query_id']\n",
    "            pid = d['product_id']\n",
    "            text = d['query']\n",
    "            label_words = d['label_words']\n",
    "            t1, t2 = token2id_X(text, x_dict=word_index, maxlen=cfg[\"maxlen\"])\n",
    "            \n",
    "            image = np.array(d['feature_convert'], dtype=\"float32\")\n",
    "            image = image[: cfg[\"max_box\"]]\n",
    "            img_mask = [1 for _ in image[: cfg[\"max_box\"]]]                   \n",
    "            pos = np.array(d['pos'], dtype=\"float32\")\n",
    "            pos = pos[: cfg[\"max_box\"]]\n",
    "            \n",
    "            image_char = [token2id_X(ent, x_dict=word_index)[0] for ent in label_words.split(IMAGE_LABEM_CONCAT_TOKEN)]\n",
    "            image_char = image_char[: cfg[\"max_box\"]]\n",
    "            image_char = pad_sequences(image_char, \n",
    "                                       maxlen=cfg[\"max_char\"], \n",
    "                                       dtype='int32',\n",
    "                                       padding='post',\n",
    "                                       truncating='post',\n",
    "                                       value=cfg[\"x_pad\"]) \n",
    "            output = model.predict([[t1], [t2], [image], [img_mask], [pos], [image_char]])\n",
    "            result[qid].append((pid, output[0][1]))\n",
    "            \n",
    "        query_id,product1,product2,product3,product4,product5 = [],[],[],[],[],[]\n",
    "        for key in result.keys():\n",
    "            rlist = result[key]\n",
    "            rlist.sort(key=lambda x: x[1], reverse=True)\n",
    "            query_id.append(key)\n",
    "            product1.append(rlist[0][0])\n",
    "            product2.append(rlist[1][0])\n",
    "            product3.append(rlist[2][0])\n",
    "            product4.append(rlist[3][0])\n",
    "            product5.append(rlist[4][0])\n",
    "        sub = pd.DataFrame({'query-id':query_id,\n",
    "                            'product1':product1,\n",
    "                            'product2':product2,\n",
    "                            'product3':product3,\n",
    "                            'product4':product4,\n",
    "                            'product5':product5,\n",
    "\n",
    "        })\n",
    "        sub.to_csv('../result/val_submission.csv',index=0)\n",
    "        \n",
    "        reference = json.load(open(VAL_ANS_PATH))\n",
    "        \n",
    "        # read predictions\n",
    "        k = 5\n",
    "        predictions = read_submission('../result/val_submission.csv', reference, k)\n",
    "\n",
    "        # compute score for each query\n",
    "        score_sum = 0.\n",
    "        for qid in reference.keys():\n",
    "            ground_truth_ids = set([str(pid) for pid in reference[qid]])\n",
    "            ref_vec = [1.0] * len(ground_truth_ids)\n",
    "            pred_vec = [1.0 if pid in ground_truth_ids else 0.0 for pid in predictions[qid]]\n",
    "            score_sum += get_ndcg(pred_vec, ref_vec, k)\n",
    "        # the higher score, the better\n",
    "        score = score_sum / len(reference)\n",
    "        \n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "fold_id = 0\n",
    "print(\"\\n\\n[!] fold_id = {} starting\".format(fold_id))\n",
    "cfg[\"num_example\"] = len_data_\n",
    "\n",
    "K.clear_session()\n",
    "# del train_data\n",
    "# gc.collect()\n",
    "\n",
    "\n",
    "seed(SEED - fold_id)\n",
    "np.random.seed(SEED - fold_id)\n",
    "tf.random.set_random_seed(SEED - fold_id)\n",
    "\n",
    "print(cfg)\n",
    "model = build_model(cfg, summary=True, word_embedding_matrix=word_embedding_matrix)\n",
    "print(\"created model\")\n",
    "model.load_weights('./model_2.h5')\n",
    "print(\"Model loaded!\")\n",
    "evaluator = Evaluate(filename=cfg[\"filename\"])\n",
    "#     checkpoint= ModelCheckpoint(cfg[\"filename\"], monitor='acc', verbose=1, save_best_only=False, mode='max') \n",
    "\n",
    "print(\"Training...\")\n",
    "train_D = data_generator(train_data)\n",
    "\n",
    "model.fit_generator(train_D.__iter__(),\n",
    "                          steps_per_epoch=train_D.steps,\n",
    "                          epochs=MAX_EPOCH,\n",
    "                          callbacks=[evaluator],\n",
    "                        \n",
    "                          shuffle=True\n",
    "                          )\n",
    "print(\"\\n\\n[!] fold_id = {} finish\".format(fold_id))\n",
    "del model, evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "with open('../data/test_data.pkl', 'rb') as outp:\n",
    "    test_data = pickle.load(outp)\n",
    "\n",
    "f =\"modelbest_model.h5\"\n",
    "\n",
    "custom_objects = get_custom_objects()\n",
    "model = load_model(f, custom_objects=custom_objects)  \n",
    "\n",
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29005, 11)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 29005/29005 [06:25<00:00, 75.26it/s]\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "result = defaultdict(list)\n",
    "for i in trange(len(test_data)):\n",
    "    d = test_data.iloc[i]\n",
    "    qid = d['query_id']\n",
    "    pid = d['product_id']\n",
    "    text = d['query']\n",
    "    label_words = d['label_words']\n",
    "    t1, t2 = token2id_X(text, x_dict=word_index, maxlen=cfg[\"maxlen\"])\n",
    "\n",
    "    image = np.array(d['feature_convert'], dtype=\"float32\")\n",
    "    image = image[: cfg[\"max_box\"]]\n",
    "    img_mask = [1 for _ in image[: cfg[\"max_box\"]]]                   \n",
    "    pos = np.array(d['pos'], dtype=\"float32\")\n",
    "    pos = pos[: cfg[\"max_box\"]]\n",
    "\n",
    "    image_char = [token2id_X(ent, x_dict=word_index)[0] for ent in label_words.split(IMAGE_LABEM_CONCAT_TOKEN)]\n",
    "    image_char = image_char[: cfg[\"max_box\"]]\n",
    "    image_char = pad_sequences(image_char, \n",
    "                               maxlen=cfg[\"max_char\"], \n",
    "                               dtype='int32',\n",
    "                               padding='post',\n",
    "                               truncating='post',\n",
    "                               value=cfg[\"x_pad\"]) \n",
    "    output = model.predict([[t1], [t2], [image], [img_mask], [pos], [image_char]])\n",
    "    result[qid].append((pid, output[0][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish loading model model_1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 14720/14720 [03:21<00:00, 73.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.43037326051446445\n",
      "finish loading model model_2.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 14720/14720 [03:29<00:00, 70.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.5059559884305539\n",
      "finish loading model model_3.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 14720/14720 [03:23<00:00, 72.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.5566925279921698\n",
      "finish loading model model_4.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 14720/14720 [03:26<00:00, 71.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0.5685182561225259\n",
      "finish loading model model_5.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 14720/14720 [03:25<00:00, 71.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 0.5810425547317598\n",
      "finish loading model model_6.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 14720/14720 [03:37<00:00, 67.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 0.5893897054041662\n",
      "finish loading model model_7.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 14720/14720 [03:37<00:00, 67.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 0.581425386758683\n",
      "finish loading model model_8.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 14720/14720 [03:44<00:00, 65.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 0.5837348830904171\n"
     ]
    }
   ],
   "source": [
    "def evaluate_valid( model_name = \"modelbest_model.h5\"):\n",
    "    custom_objects = get_custom_objects()\n",
    "    model = load_model(model_name, custom_objects=custom_objects)  \n",
    "\n",
    "    print(\"finish loading model\", model_name)\n",
    "    result = defaultdict(list)\n",
    "    for i in trange(len(val_data)):\n",
    "        d = val_data.iloc[i]\n",
    "        qid = d['query_id']\n",
    "        pid = d['product_id']\n",
    "        text = d['query']\n",
    "        label_words = d['label_words']\n",
    "        t1, t2 = token2id_X(text, x_dict=word_index, maxlen=cfg[\"maxlen\"])\n",
    "\n",
    "        image = np.array(d['feature_convert'], dtype=\"float32\")\n",
    "        image = image[: cfg[\"max_box\"]]\n",
    "        img_mask = [1 for _ in image[: cfg[\"max_box\"]]]                   \n",
    "        pos = np.array(d['pos'], dtype=\"float32\")\n",
    "        pos = pos[: cfg[\"max_box\"]]\n",
    "\n",
    "        image_char = [token2id_X(ent, x_dict=word_index)[0] for ent in label_words.split(IMAGE_LABEM_CONCAT_TOKEN)]\n",
    "        image_char = image_char[: cfg[\"max_box\"]]\n",
    "        image_char = pad_sequences(image_char, \n",
    "                                   maxlen=cfg[\"max_char\"], \n",
    "                                   dtype='int32',\n",
    "                                   padding='post',\n",
    "                                   truncating='post',\n",
    "                                   value=cfg[\"x_pad\"]) \n",
    "        output = model.predict([[t1], [t2], [image], [img_mask], [pos], [image_char]])\n",
    "        result[qid].append((pid, output[0][1]))\n",
    "\n",
    "    query_id,product1,product2,product3,product4,product5 = [],[],[],[],[],[]\n",
    "    for key in result.keys():\n",
    "        rlist = result[key]\n",
    "        rlist.sort(key=lambda x: x[1], reverse=True)\n",
    "        query_id.append(key)\n",
    "        product1.append(rlist[0][0])\n",
    "        product2.append(rlist[1][0])\n",
    "        product3.append(rlist[2][0])\n",
    "        product4.append(rlist[3][0])\n",
    "        product5.append(rlist[4][0])\n",
    "    sub = pd.DataFrame({'query-id':query_id,\n",
    "                        'product1':product1,\n",
    "                        'product2':product2,\n",
    "                        'product3':product3,\n",
    "                        'product4':product4,\n",
    "                        'product5':product5,\n",
    "\n",
    "    })\n",
    "    sub.to_csv('../result/val_submission.csv',index=0)\n",
    "\n",
    "    reference = json.load(open(VAL_ANS_PATH))\n",
    "\n",
    "    # read predictions\n",
    "    k = 5\n",
    "    predictions = read_submission('../result/val_submission.csv', reference, k)\n",
    "\n",
    "    # compute score for each query\n",
    "    score_sum = 0.\n",
    "    for qid in reference.keys():\n",
    "        ground_truth_ids = set([str(pid) for pid in reference[qid]])\n",
    "        ref_vec = [1.0] * len(ground_truth_ids)\n",
    "        pred_vec = [1.0 if pid in ground_truth_ids else 0.0 for pid in predictions[qid]]\n",
    "        score_sum += get_ndcg(pred_vec, ref_vec, k)\n",
    "    # the higher score, the better\n",
    "    score = score_sum / len(reference)\n",
    "\n",
    "    return score\n",
    "for i in range(1, 9):\n",
    "    print(i, evaluate_valid(f\"model_{i}.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_id,product1,product2,product3,product4,product5 = [],[],[],[],[],[]\n",
    "for key in result.keys():\n",
    "    rlist = result[key]\n",
    "    rlist.sort(key=lambda x: x[1], reverse=True)\n",
    "    query_id.append(key)\n",
    "    product1.append(rlist[0][0])\n",
    "    product2.append(rlist[1][0])\n",
    "    product3.append(rlist[2][0])\n",
    "    product4.append(rlist[3][0])\n",
    "    product5.append(rlist[4][0])\n",
    "\n",
    "sub = pd.DataFrame({'query-id':query_id,\n",
    "                    'product1':product1,\n",
    "                    'product2':product2,\n",
    "                    'product3':product3,\n",
    "                    'product4':product4,\n",
    "                    'product5':product5,\n",
    "\n",
    "})\n",
    "\n",
    "sub.to_csv('../result/submission.csv',index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query-id</th>\n",
       "      <th>product1</th>\n",
       "      <th>product2</th>\n",
       "      <th>product3</th>\n",
       "      <th>product4</th>\n",
       "      <th>product5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>103017156</td>\n",
       "      <td>103019797</td>\n",
       "      <td>103060348</td>\n",
       "      <td>103007358</td>\n",
       "      <td>103048369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>103013182</td>\n",
       "      <td>103049334</td>\n",
       "      <td>103026445</td>\n",
       "      <td>103040915</td>\n",
       "      <td>103042631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>103060996</td>\n",
       "      <td>103021636</td>\n",
       "      <td>103055828</td>\n",
       "      <td>103038482</td>\n",
       "      <td>103020842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>103031918</td>\n",
       "      <td>103030069</td>\n",
       "      <td>103023997</td>\n",
       "      <td>103052105</td>\n",
       "      <td>103060816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>103057012</td>\n",
       "      <td>103039172</td>\n",
       "      <td>103061125</td>\n",
       "      <td>103015434</td>\n",
       "      <td>103025374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query-id   product1   product2   product3   product4   product5\n",
       "0         0  103017156  103019797  103060348  103007358  103048369\n",
       "1         1  103013182  103049334  103026445  103040915  103042631\n",
       "2         2  103060996  103021636  103055828  103038482  103020842\n",
       "3         3  103031918  103030069  103023997  103052105  103060816\n",
       "4         4  103057012  103039172  103061125  103015434  103025374"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done at 2020-06-10 17:16:13.366156\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(\"Done at\", datetime.datetime.now()) \n",
    "\n",
    "## repredict with modelbest_mode.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
